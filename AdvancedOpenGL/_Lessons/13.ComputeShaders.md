# Compute shaders

Compute shaders are a way to take advantage of the enormous computational power of graphics processors that implement OpenGL. Just like all shaders in OpenGL, they are written in GLSL and run in large parallel groups that simultaneously work on huge amounts of data. In addition to the facilities available to other shaders such as texturing, storage buffers, and atomic memory operations, compute shaders are able to synchronize with one another and share data among themselves to make general computation easier. They stand apart from the rest of the OpenGL pipeline and are designed to provide as much flexibility to the application developer as possible. In this lesson, we discuss compute shaders, examine their similarities and their differences to other shader types in OpenGL, and explain some of their unique properties and abilities.

## Using Compute Shaders

### Compile

Modern graphics processors are extremely powerful devices capable of performing a huge amount of numeric calculation. The compute shader stage is effectively its own pipeline, somewhat disconnected from the rest of OpenGL. It has no fixed inputs or outputs, does not interface with any of the fixed-function pipeline stages, is very flexible, and has capabilities that other stages do not possess.

Having said this, a compute shader is just like any other shader from a programming point of view. It is written in GLSL, represented as a shader object, and linked into a program object. When you create a compute shader, you call glCreateShader() and pass the GL_COMPUTE_SHADER parameter as the shader type. You get back a new shader object from this call that you can use to load your shader code with glShaderSource(), compile with glCompileShader(), and attach to a program object with glAttachShader(). Then, you link the program object by calling glLinkProgram(), just as you would with any graphics program.

You can’t mix and match compute shaders with shaders of other types. For example, you can’t attach a compute shader to a program object that also has a vertex or fragment shader attached to it and then link the program object. If you attempt this, the link will fail. Thus, a linked program object can contain only compute shaders or only graphics shaders (vertex, tessellation, geometry, or fragment), but not a combination of the two. We will sometimes refer to a linked program object that contains compute shaders (and so only compute shaders) as a compute program (as opposed to a graphics program, which contains only graphics shaders).

Example code to compile and link our do-nothing compute shader:

```
GLuint      compute_shader;
GLuint      compute_program;

static const GLchar * compute_source[] =
{
    '#version 450 core                                       \n'
    '                                                        \n'
    'layout (local_size_x = 32, local_size_y = 32) in;       \n'
    '                                                        \n'
    'void main(void)                                         \n'
    '{                                                       \n'
    '   // Do nothing                                        \n'
    '}                                                       \n'
};

// Create a shader, attach source, and compile.
compute_shader = glCreateShader(GL_COMPUTE_SHADER);
glShaderSource(compute_shader, 1, compute_source, NULL);
glCompileShader(compute_shader);

// Create a program, attach shader, link.
compute_program = glCreateProgram();
glAttachShader(compute_program, compute_shader);
glLinkProgram(compute_program);

// Delete shader because we're done with it.
glDeleteShader(compute_shader);
```

Once you have run the code, you will have a ready-to-run compute program in compute_program. A compute program can use uniforms, uniform blocks, shader storage blocks, and so on, just as any other program does. You also make it current by calling glUseProgram(). Once it is the current program object, functions such as glUniform*() affect its state as normal.

###  Execute compute shaders

Once you have made a compute program current, and set up any resources that it might need access to, you need to actually execute it. To do this, we have a pair of functions:

```
void glDispatchCompute(GLuint num_groups_x,
                       GLuint num_groups_y,
                       GLuint num_groups_z);
```
and
```
void glDispatchComputeIndirect(GLintptr indirect);
```

The glDispatchComputeIndirect() function is to glDispatchCompute() as glDrawArraysIndirect() is to glDrawArraysInstancedBaseInstance(). That is, the indirect parameter is interpreted as an offset into a buffer object that contains a set of parameters that could be passed to glDispatchCompute(). In code, this structure would look like this:

```
typedef struct {
    GLuint num_groups_x;
    GLuint num_groups_y;
    GLuint num_groups_z;
} DispatchIndirectCommand;
```

However, we need to understand how these parameters are interpreted to use them effectively.

#### Global and Local Work Groups

Compute shaders execute in what are called work groups. A single call to glDispatchCompute() or glDispatchComputeIndirect() will cause a single global work group1 to be sent to OpenGL for processing. That global work group will then be subdivided into a number of local work groups—the number of local work groups in each of the x, y, and z dimensions is set by the num_groups_x, num_groups_y, and num_groups_z parameters, respectively. A work group is fundamentally a 3D block of work items, where each work item is processed by an invocation of a compute shader running your code. The size of each local work group in the x, y, and z dimensions is set using an input layout qualifier in your shader source code. You can see an example of this in the simple compute shader that we introduced earlier:
```
layout (local_size_x = 4,
        local_size_y = 7,
        local_size_z = 10) in;
```

In this example, the local work group size would be 4 × 7 × 10 work items or invocations, for a total of 280 work items per local work group. The maximum size of a work group can be found by querying the values of two parameters: GL_MAX_COMPUTE_WORK_GROUP_SIZE and GL_MAX_COMPUTE_WORK_GROUP_INVOCATIONS. For the first of these, you query it using the glGetIntegeri_v() function, passing it as the target parameter and 0, 1, or 2 as the index parameter to specify the x, y, or z dimension, respectively. The maximum size will be at least 1024 items in the x and y dimensions and 64 items in the z dimension. The value you get by querying the GL_MAX_COMPUTE_WORK_GROUP_INVOCATIONS constant is the maximum total number of invocations allowed in a single work group, which is the maximum allowed product of the x, y, and z dimensions, or the volume of the local work group. That value will be at least 1024 items.

It’s possible to launch 1D or 2D work groups by simply setting either the y or z dimension (or both) to 1. In fact, the default size in all dimensions is 1. Thus, if you don’t include them in your input layout qualifier, you will create a work group size of lower dimension than 3. For example,

```
layout (local_size_x = 512) in;
```

will create a 1D local work group of 512 (× 1 × 1) items and

```
layout (local_size_x = 64, local_size_y = 64) in;
```

will create a 2D local work group of 64 × 64 (× 1) items. The local work group size is used when you link the program to determine the size and dimensions of the work groups executed by the program. You can find the local work group size of a program’s compute shaders by calling glGetProgramiv() with pname set to GL_COMPUTE_WORK_GROUP_SIZE. It will return three integers giving the size of the work groups. For example, you could write

```
int size[3];

glGetProgramiv(program, GL_COMPUTE_WORK_GROUP_SIZE, size);

printf('Work group size is %d x %d x %d items.\n', size[0], size[1], size[2]);
```

Once you have defined a local work group size, you can dispatch a 3D block of workgroups to do work for you. The size of this block is specified by the num_groups_x, num_groups_y, and num_groups_z parameters to glDispatchCompute() or the equivalent members of the DispatchIndirectCommand structure stored in the buffer object bound to the GL_DISPATCH_INDIRECT_BUFFER target. This block of local work groups is known as the global work group and its dimension doesn’t need to be the same as the dimension of the local work group. That is, you could dispatch a 3D global work group of 1D local work groups, a 2D global work group of 3D local work groups, and so on.

#### Compute Shader Inputs and Outputs

First and foremost, compute shaders have no built-in outputs. They have no built-in outputs at all, and you cannot declare any user-defined outputs as you are able to do in other shader stages. This is because the compute shader forms a kind of single-stage pipeline with nothing before it and nothing after it. However, like some of the graphics shaders, it does have a few built-in input variables that you can use to determine where you are in your local work group and within the greater global work group.

The first variable, gl_LocalInvocationID, is the index of the shader invocation within the local work group. It is implicitly declared as a uvec3 input to the shader and each element ranges in value from 0 to 1 less than the local work group size in the corresponding dimension (x, y, or z). The local work group size is stored in the gl_WorkGroupSize variable, which is also implicitly declared as a uvec3 type. Again, even if you declare your local work group size to be 1D or 2D, the work group will still essentially be 3D, but with the sizes of the unused dimensions set to 1. That is, gl_LocalInvocationID and gl_WorkGroupSize will still be implicitly declared as uvec3 variables, but the y and z components of gl_LocalInvocationID will be 0 and the corresponding components for gl_WorkGroupSize will be 1.

Just as gl_WorkGroupSize and gl_LocalInvocationID store the size of the local work group and the location of the current shader invocation within the work group, so gl_NumWorkGroups and gl_WorkGroupID contain the number of work groups and the index of the current work group within the global set, respectively. Again, both are implicitly declared as uvec3 variables. The value of gl_NumWorkGroups is set by the glDispatchCompute() or glDispatchComputeIndirect() commands and contains the values of num_groups_x, num_groups_y, and num_groups_z in its three elements. The elements of gl_WorkGroupID range in value from 0 to 1 less than the values of the corresponding elements of gl_NumWorkGroups.

These variables are illustrated in next figure. The diagram shows a global work group that contains three work groups in the x dimension, four work groups in the y dimension, and eight work groups in the z dimension. Each local work group is a 2D array of work items that contains six items in the x dimension and four items in the y dimension.

![](13.01.PNG)

Between gl_WorkGroupID and gl_LocalInvocationID, you can tell where in the complete set of work items your current shader invocation is located. Likewise, between gl_NumWorkGroups and gl_WorkGroupSize, you can figure out the total number of invocations in the global set. However, OpenGL provides the global invocation index to you through the gl_GlobalInvocationID built-in variable. It is effectively calculated as

```
gl_GlobalInvocationID = gl_WorkGroupID * gl_WorkGroupSize + gl_LocalInvocationID;
```

Finally, the gl_LocalInvocationIndex built-in variable contains a “flattened” form of gl_LocalInvocationID. That is, the 3D variable is converted to a 1D index using the following code:

```
gl_LocalInvocationIndex =
    gl_LocalInvocationID.z * gl_WorkGroupSize.x * gl_WorkGroupSize.y +
    gl_LocalInvocationID.y * gl_WorkGroupSize.x +
    gl_LocalInvocationID.x;
```

The values stored in these variables allow your shader to know where it is in the local and global work groups and can be used as indices into arrays of data, texture coordinates, or random seeds, or for any other purpose.

Now we come to outputs. We started this section by stating that compute shaders have no outputs. That’s true, but it doesn’t mean that compute shaders can’t output any data—it just means that there are no fixed outputs represented by built-in output variables, for example. Compute shaders can still produce data, but the data must be stored into memory explicitly by your shader code. For instance, in your compute shader you could write into a shader storage block, use image functions such as imageStore or atomics, or increment and decrement the values of atomic counters. These operations have side effects, which means that their operation can be detected because they update the contents of memory or otherwise have externally visible consequences.

Consider the following shader, which reads from one image, logically inverts the data, and writes the data back out to another image.

```
#version 450 core

layout (local_size_x = 32,
        local_size_y = 32) in;

layout (binding = 0, rgba32f) uniform image2D img_input;
layout (binding = 1) uniform image2D img_output;

void main(void)
{
    vec4 texel;
    ivec2 p = ivec2(gl_GlobalInvocationID.xy);

    texel = imageLoad(img_input, p);
    texel = vec4(1.0) - texel;
    imageStore(img_output, p, texel);
}
```

To execute this shader, we would compile it and link it into a program object and then set up our images by binding a level of a texture object to each of the first two image units. As you can see from previous, the local work group size is 32 invocations in x and y, so our images should ideally be integer multiples of 32 texels wide and high. Once the images are bound, we can call glDispatchCompute(), setting the num_groups_x and num_groups_y parameters to the width and height of the images divided by 32, respectively, and num_groups_z to 1. Code to do this is shown here:

```
// Bind input image
glBindImageTexture(0, tex_input, 0, GL_FALSE,0, GL_READ_ONLY, GL_RGBA32F);

// Bind output image
glBindImageTexture(1, tex_output, 0, GL_FALSE, 0, GL_WRITE_ONLY, GL_RGBA32F);

// Dispatch the compute shader
glDispatchCompute(IMAGE_WIDTH / 32, IMAGE_HEIGHT / 32, 1);
```

### Compute Shader communication

#### Shared variables

Compute shaders execute on work items in work groups much as tessellation control shaders execute on control points in patches — both work groups and patches are created from groups of invocations. Within a single patch, tessellation control shaders can write to variables qualified with the patch storage qualifier and, if they are synchronized correctly, read the values that other invocations in the same patch wrote to them. As such, this allows a limited form of communication between the tessellation control shader invocations in a single patch. However, this comes with substantial limitations — for example, the amount of storage available for patch qualified variables is fairly limited, and the number of control points in a single patch is quite small.

Compute shaders provide a similar mechanism, but offer significantly more flexibility and power. Just as you can declare variables with the patch storage qualifier in a tessellation control shader, so you can declare variables with the shared storage qualifier, which allows them to be shared between compute shader invocations running in the same local work group. Variables declared with the shared storage qualifier are known as shared variables. Access to shared variables is generally much faster than access to main memory through images or storage blocks. Thus, if you expect multiple invocations of your compute shader to access the same data, it makes sense to copy the data from main memory into a shared variable (or an array of them), access the data from there, possibly updating it in place, and then write any results back to main memory when you’re done.

Keep in mind, though, that you can use only a limited number of shared variables. A modern graphics board might have several gigabytes of main memory, whereas the amount of shared variable storage space might be limited to just a few kilobytes. The amount of shared memory available to a compute shader can be determined by calling glGetIntegerv() with pname set to GL_MAX_COMPUTE_SHARED_MEMORY_SIZE. The minimum amount of shared memory required to be supported in OpenGL is only 32 kilobytes, so while your implementation may have more than this, you shouldn’t count on it being substantially larger.

#### Synchronizing Compute Shaders

The invocations in a work group will most likely run in parallel—this is where the vast computation power of graphics processors comes from. The processor will likely divide each local work group into a number of smaller chunks (chunk sizes of 16, 32, or 64 elements are common.), executing the invocations in a single chunk in lockstep. These chunks are then time-sliced onto the processor’s computational resources and those timeslices may be assigned in any order. It may be that a chunk of invocations is completed before any other chunks from the same local work group begin, but more than likely there will be many “live” chunks present on the processor at any given time.

Because these chunks can effectively run out of order but are allowed to communicate, we need a way to ensure that messages received by a recipient are the most recent ones sent. Imagine if you were told to go to someone’s office and perform the duty written on that individual’s whiteboard. Each day, this person writes a new message on the whiteboard, but you don’t know at what time that happens. When you go into the office, how do you know if the message on the whiteboard is what you’re supposed to do, or if it’s left over from the previous day? You’d be in a bit of trouble. Now suppose the owner of the office left the door locked until he or she had been there and written the message. If you showed up and the door was locked, you’d have to wait outside the office. This is known as a barrier. If the door is open, you can look at the message; if it’s locked, you need to wait until the person arrives to open it.

A similar mechanism is available to compute shaders: the barrier() function, which executes a flow control barrier. When you call barrier() in your compute shader, it will be blocked until all other shader invocations in the same local work group have reached that point in the shader. In a time-slicing architecture, executing the barrier() function means that your shader (along with the chunk it’s in) will give up its timeslice so that another invocation can execute until it reaches the barrier. Once all the other invocations in the local work group reach the barrier (or if they have already gotten there before your invocation arrives) execution continues as usual.

Flow control barriers are important when shared memory is in use because they allow you to know when other shader invocations in the same local work group have reached the same point as the current invocation. If the current invocation has written to some shared memory variable, then you know that all the others must have written to theirs as well, so it’s safe to read the data they wrote. Without a barrier, you would have no idea whether data that was supposed to have been written to shared variables actually has been. At best, you’d leave your application susceptible to race conditions; at worst, the application wouldn’t work at all. Consider, for example, the shader shown here:

```
#version 450 core

layout (local_size_x = 1024) in;

layout (binding = 0, r32ui) uniform uimageBuffer image_in;

layout (binding = 1) uniform uimageBuffer image_out;

shared uint temp_storage[1024];

void main(void)
{
    // Load from the input image
    uint n = imageLoad(image_in, gl_LocalInvocationID.x).x;

    // Store into shared storage
    temp_storage[gl_LocalInvocationID.x] = n;

    // Uncomment this to avoid the race condition
    // barrier();
    // memoryBarrierShared();

    // Read the data written by the invocation 'to the left'
    n = temp_storage[(gl_LocalInvocationID.x - 1) & 1023];

    // Write new data into the buffer
    imageStore(image_out, gl_LocalInvocationID.x, n);
}
```

This shader loads data from a buffer image into a shared variable. Each invocation of the shader loads a single item from the buffer and writes it into its own “slot” in the shared variable array. Then, it reads from the slot owned by the invocation to its left and writes the data out to the buffer image. The result should be that the data in the buffer is moved along by one element. However, following figures illustrates what actually happens.

![](13.02.PNG)

As you can see in the figure, multiple shader invocations have been time-sliced onto a single computational resource. At t0, invocation A runs the first couple of lines of the shader and writes its value to temp_storage. At t1, invocation B runs a line and then at t2, invocation C takes over and runs the same first two lines of the shader. At time t3, A gets its timeslice back again and completes the shader. It’s done at this point, but the other invocations haven’t finished their work yet. At t4, invocation D finally gets a turn but is quickly interrupted by invocation C, which reads from temp_storage. Now we have a problem: Invocation C was expecting to read data from the shared storage that was written by invocation B, but invocation B hasn’t reached that point in the shader yet! Execution continues blindly, and invocations D, C, and B all finish the shader, but the data stored by C will be garbage.

This is known as a race condition. The shader invocations race each other to the same point in the shader, and some invocations will read from the temp_storage shared variable before others have written their data into it. The result is that they pick up stale data, which then gets written into the output buffer image. Uncommenting the call to barrier() in code produces an execution flow more like that shown here:

![](13.03.PNG)

Compare figures. Both depict four shader invocations being time-sliced onto the same computational resource, except that second figure does not exhibit the race condition. In second figure, we again start with shader invocation A executing the first couple of lines of the shader, but then it calls the barrier() function, which causes it to yield its timeslice. Next, invocation B executes the first couple of lines and then is preempted. Then, C executes the shader as far as the barrier() function and so yields. Invocation B executes its barrier but gets no further because D still has not reached the barrier function. Finally, invocation D gets a chance to run reads from the image buffer, writes its data into the shared storage area, and then calls barrier(). This signals all the other invocations that it is safe to continue running.

Immediately after invocation D executes the barrier, all other invocations are able to run again. Invocation C loads from the shared storage, then D, and C and D both store their results to the image. Finally invocations A and B read from the shared storage and write their results out to memory. As you can see, no invocation tried to read data that hasn’t been written yet. The presence of the barrier() functions affected the scheduling of the invocations with respect to one another. Although these diagrams show only four invocations competing for a single resource, in real OpenGL implementations there are likely to be many hundreds of threads competing for perhaps a few tens of resources. As you might guess, the likelihood of data corruption due to race conditions is much higher in these scenarios.

## Examples

